{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install proteinshake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ProteinShake Point Cloud Implementation\n",
    "Adapts SequenceLSTM model to work with ProteinShake's testing framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import sys\n",
    "from typing import List, Tuple, Optional\n",
    "from proteinshake.tasks import EnzymeClassTask, DummyModel\n",
    "\n",
    "def print_immediate(msg):\n",
    "    print(msg)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def collate_point_clouds(batch):\n",
    "    point_clouds = []\n",
    "    labels = []\n",
    "\n",
    "    for point_cloud, metadata in batch:\n",
    "        point_clouds.append(point_cloud)\n",
    "        ec_class = metadata['protein']['EC']\n",
    "        label = int(ec_class.split('.')[0]) - 1\n",
    "        labels.append(label)\n",
    "\n",
    "    max_points = max(pc.shape[0] for pc in point_clouds)\n",
    "    padded_clouds = []\n",
    "    masks = []\n",
    "\n",
    "    for pc in point_clouds:\n",
    "        pad_length = max_points - pc.shape[0]\n",
    "        if pad_length > 0:\n",
    "            padding = torch.zeros(pad_length, pc.shape[1])\n",
    "            padded_pc = torch.cat([pc, padding], dim=0)\n",
    "        else:\n",
    "            padded_pc = pc\n",
    "\n",
    "        mask = torch.ones(pc.shape[0])\n",
    "        if pad_length > 0:\n",
    "            mask = torch.cat([mask, torch.zeros(pad_length)])\n",
    "        padded_clouds.append(padded_pc)\n",
    "        masks.append(mask)\n",
    "\n",
    "    batch_clouds = torch.stack(padded_clouds)\n",
    "    batch_masks = torch.stack(masks)\n",
    "    batch_labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return batch_clouds, batch_masks, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointCloudToSequenceAdapter(nn.Module):\n",
    "    def __init__(self, point_cloud_dim: int = 4, sequence_embed_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.point_conv1 = nn.Conv1d(point_cloud_dim, 64, 1)\n",
    "        self.point_conv2 = nn.Conv1d(64, 128, 1)\n",
    "        self.point_conv3 = nn.Conv1d(128, sequence_embed_dim, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(sequence_embed_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, point_cloud, mask=None):\n",
    "        batch_size, num_points, point_dim = point_cloud.shape\n",
    "        x = point_cloud.transpose(1, 2)\n",
    "        x = F.relu(self.bn1(self.point_conv1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn2(self.point_conv2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.bn3(self.point_conv3(x)))\n",
    "        x = x.transpose(1, 2)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(-1)\n",
    "            x = x * mask\n",
    "        return x\n",
    "\n",
    "class SequenceLSTMBaseline(nn.Module):\n",
    "    def __init__(self, num_classes: int = 7, hidden_dim: int = 128, max_length: int = 500, \n",
    "                 input_embed_dim: int = 128, num_layers: int = 2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=0.3 if num_layers > 1 else 0\n",
    "        )\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim * 2,\n",
    "            num_heads=8,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, num_classes)\n",
    "        )\n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        print_immediate(f\"SequenceLSTM initialized with {total_params:,} parameters\")\n",
    "\n",
    "    def forward(self, embedded_sequences, mask=None):\n",
    "        lstm_out, _ = self.lstm(embedded_sequences)\n",
    "        if mask is not None:\n",
    "            mask_expanded = mask.unsqueeze(-1)\n",
    "            lstm_out = lstm_out * mask_expanded\n",
    "        attn_mask = ~mask.bool() if mask is not None else None\n",
    "        attended, _ = self.attention(lstm_out, lstm_out, lstm_out, key_padding_mask=attn_mask)\n",
    "        if mask is not None:\n",
    "            mask_expanded = mask.unsqueeze(-1)\n",
    "            attended = attended * mask_expanded\n",
    "            pooled = attended.sum(dim=1) / mask.sum(dim=1, keepdim=True).clamp(min=1)\n",
    "        else:\n",
    "            pooled = attended.mean(dim=1)\n",
    "        logits = self.classifier(pooled)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ProteinShake Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinShakePointLSTMModel(DummyModel):\n",
    "    def __init__(self, task, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        super().__init__(task)\n",
    "        self.device = device\n",
    "        sample_data, _ = task.train[0]\n",
    "        point_cloud_shape = sample_data.shape\n",
    "        print_immediate(f\"Point cloud shape: {point_cloud_shape}\")\n",
    "        print_immediate(f\"Using device: {device}\")\n",
    "        self.point_adapter = PointCloudToSequenceAdapter(\n",
    "            point_cloud_dim=point_cloud_shape[-1],\n",
    "            sequence_embed_dim=64\n",
    "        )\n",
    "        self.sequence_model = SequenceLSTMBaseline(\n",
    "            num_classes=task.num_classes,\n",
    "            hidden_dim=64,\n",
    "            max_length=1000,\n",
    "            input_embed_dim=64,\n",
    "            num_layers=1\n",
    "        )\n",
    "        self.point_adapter.to(device)\n",
    "        self.sequence_model.to(device)\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            list(self.point_adapter.parameters()) + list(self.sequence_model.parameters()),\n",
    "            lr=0.001,\n",
    "            weight_decay=1e-5\n",
    "        )\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        total_params = (sum(p.numel() for p in self.point_adapter.parameters()) +\n",
    "                       sum(p.numel() for p in self.sequence_model.parameters()))\n",
    "        print_immediate(f\"Total model parameters: {total_params:,}\")\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        self.point_adapter.train()\n",
    "        self.sequence_model.train()\n",
    "        point_clouds, masks, labels = batch\n",
    "        point_clouds = point_clouds.to(self.device)\n",
    "        masks = masks.to(self.device)\n",
    "        labels = labels.to(self.device)\n",
    "        self.optimizer.zero_grad()\n",
    "        sequence_embeddings = self.point_adapter(point_clouds, masks)\n",
    "        logits = self.sequence_model(sequence_embeddings, masks)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return {\n",
    "            'loss': loss.item(),\n",
    "            'accuracy': (logits.argmax(dim=1) == labels).float().mean().item()\n",
    "        }\n",
    "\n",
    "    def test_step(self, test_data):\n",
    "        self.point_adapter.eval()\n",
    "        self.sequence_model.eval()\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            test_loader = DataLoader(test_data, batch_size=8, shuffle=False, collate_fn=collate_point_clouds)\n",
    "            for batch in test_loader:\n",
    "                point_clouds, masks, labels = batch\n",
    "                point_clouds = point_clouds.to(self.device)\n",
    "                masks = masks.to(self.device)\n",
    "                sequence_embeddings = self.point_adapter(point_clouds, masks)\n",
    "                logits = self.sequence_model(sequence_embeddings, masks)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                predictions.extend(preds.cpu().numpy())\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_proteinshake_point_model():\n",
    "    print_immediate(\"Loading ProteinShake EnzymeClassTask with point cloud representation...\")\n",
    "    task = EnzymeClassTask().to_point().torch()\n",
    "    print_immediate(\"Task loaded successfully!\")\n",
    "    print_immediate(f\"Number of classes: {task.num_classes}\")\n",
    "    print_immediate(f\"Train set size: {len(task.train)}\")\n",
    "    print_immediate(f\"Test set size: {len(task.test)}\")\n",
    "    print_immediate(\"Creating model...\")\n",
    "    model = ProteinShakePointLSTMModel(task)\n",
    "    print_immediate(\"Starting training...\")\n",
    "    num_epochs = 10\n",
    "    batch_size = 8\n",
    "    train_loader = DataLoader(task.train, batch_size=batch_size, shuffle=True, collate_fn=collate_point_clouds)\n",
    "    for epoch in range(num_epochs):\n",
    "        print_immediate(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        epoch_losses = []\n",
    "        epoch_accs = []\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            metrics = model.train_step(batch)\n",
    "            epoch_losses.append(metrics['loss'])\n",
    "            epoch_accs.append(metrics['accuracy'])\n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        avg_acc = np.mean(epoch_accs)\n",
    "        print_immediate(f\"Epoch {epoch+1} Summary - Loss: {avg_loss:.4f} | Accuracy: {avg_acc:.4f}\")\n",
    "    print_immediate(\"\\nTraining completed! Starting evaluation...\")\n",
    "    prediction = model.test_step(task.test)\n",
    "    metrics = task.evaluate(task.test_targets, prediction)\n",
    "    print_immediate(\"\\nFinal Evaluation Results:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print_immediate(f\"  {metric_name}: {metric_value}\")\n",
    "    return metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_proteinshake_point_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}